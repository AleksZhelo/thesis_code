[meta]
; master_settings_file = 'conf_master.ini'

[model]
use_cuda = true
input_size = 120
; all models should share the same features

;;;;;;;;;;;;;;;;;;;;;;;; LSTM ;;;;;;;;;;;;;;;;;;;;;;;;

[LSTM_FC_base]
lstm_hidden_size = 160
lstm_layers = 4
fc_size = [384]
bidirectional = true
init_type = 'xavier_normal'
init_param = 1
; all lstm_fc models should share the same structure for weight reusability

[SiameseLSTM]
dropout = 0.3
fc_dropout = [0]
output_size = 64

[LSTMClassifier]
dropout = 0.3
output_size = 230

;;;;;;;;;;;;;;;;;;;;;;;; GRU ;;;;;;;;;;;;;;;;;;;;;;;;

[GRU_FC_base]
gru_hidden_size = 160
gru_layers = 4
fc_size = [384]
bidirectional = true
init_type = 'xavier_normal'
init_param = 1
use_custom = false
custom_dropout_last = false
custom_dropout_hidden = false
custom_reverse_backward_output = true
custom_mode = CONCAT
custom_decov_output = [0, 0, 0]
custom_decov_hidden = [0, 0, 0]
; all gru_fc models should share the same structure for weight reusability

[SiameseGRU]
dropout = 0.3
fc_dropout = [0]
output_size = 64

[GRUClassifier]
dropout = 0.3
output_size = 230

;;;;;;;;;;;;;;;;;;;;;;;; Training ;;;;;;;;;;;;;;;;;;;;;;;;

[general_training]
use_gru = true
noise_multiplier = 1.0
noise_prob = 0.5
mean_subtraction = false
variance_normalization = false
supplement_rare_with_noisy = false
supplement_seed = 113

[siamese_training]
batch_size = 80
learning_rate = 1e-3
train_epochs = 100

num_other = 10
loss_margin = 0.5
euclidean_loss_margin = 0.2
loss = margin_loss

lr_schedule = true
augment_parts = false

online_batch_size = 870
online_examples_per_word = 5

lr_margin_beta = 1e-3
margin_cutoff = 0.5
margin_nonzero_loss_cutoff = 1.4
margin_init_beta = 1.2
margin_nu = 0.1

[classifier_training]
batch_size = 768
learning_rate = 15e-4
train_epochs = 100
lr_schedule = true