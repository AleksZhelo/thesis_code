[meta]
; master_settings_file = 'conf_master.ini'

[model]
use_cuda = true
input_size = 120
; all models should share the same features

;;;;;;;;;;;;;;;;;;;;;;;; LSTM ;;;;;;;;;;;;;;;;;;;;;;;;

[LSTM_FC_base]
lstm_hidden_size = 160
lstm_layers = 4
fc_size = [384]
bidirectional = true
init_type = 'xavier_normal'
init_param = 1

[SiameseLSTM]
; between RNN layers
dropout = 0.3
fc_dropout = [0]
output_size = 64

[LSTMClassifier]
; between RNN layers
dropout = 0.3
output_size = 230

;;;;;;;;;;;;;;;;;;;;;;;; GRU ;;;;;;;;;;;;;;;;;;;;;;;;

[GRU_FC_base]
gru_hidden_size = 160
gru_layers = 4
fc_size = [384]
bidirectional = true
init_type = 'xavier_normal'
init_param = 1
; set to true for variational dropout (slow)
use_custom = false
; dropout after last RNN layer
custom_dropout_last = false
custom_dropout_hidden = false
; don't change
custom_reverse_backward_output = true
; don't change
custom_mode = CONCAT
custom_decov_output = [0, 0, 0]
custom_decov_hidden = [0, 0, 0]

[SiameseGRU]
; between RNN layers
dropout = 0.3
fc_dropout = [0]
output_size = 64

[GRUClassifier]
; between RNN layers
dropout = 0.3
output_size = 230

;;;;;;;;;;;;;;;;;;;;;;;; Training ;;;;;;;;;;;;;;;;;;;;;;;;

[general_training]
use_gru = true
; online noise augmentation: noisy data is sampled anew for each batch
noise_multiplier = 1.0
noise_prob = 0.5
; not used for the final network
mean_subtraction = false
variance_normalization = false
; offline noise augmentation: prepare noisy data up-front and add to the dataset
supplement_rare_with_noisy = false
supplement_seed = 113

[siamese_training]
; triplet loss params
batch_size = 80
num_other = 10

learning_rate = 1e-3
train_epochs = 100

; cosine distance margin
loss_margin = 0.5
euclidean_loss_margin = 0.2

loss = margin_loss
lr_schedule = true
augment_parts = false

; margin loss batch size
online_batch_size = 870
online_examples_per_word = 5

; margin loss params
lr_margin_beta = 1e-3
margin_cutoff = 0.5
margin_nonzero_loss_cutoff = 1.4
margin_init_beta = 1.2
margin_nu = 0.1

[classifier_training]
batch_size = 768
learning_rate = 15e-4
train_epochs = 100
lr_schedule = true